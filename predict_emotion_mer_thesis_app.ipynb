{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_emotion_mer_thesis_app.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNROFZ0mqm4VUJjZk5Jgyiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HonzaCuhel/mer-thesis-app/blob/main/predict_emotion_mer_thesis_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coKAZLXvspj9"
      },
      "source": [
        "# Multimodal Speech Emotion Recognition - Demo app\n",
        "<hr/>\n",
        "<b>Description:</b> This notebook contains a demo application of  Emotion Recognition models trained on the IEMOCAP dataset using 4 basic emotions.<br/>\n",
        "<b>Model Architecture:</b> Electra small (TER), TRILL (SER), YAMNet (SER), Electra + TRILL (MER), Electra + YAMNet (MER)<br/>\n",
        "<b>Author:</b> Jan ƒåuhel<br/>\n",
        "<b>Date:</b> 5.5.2021<br/>\n",
        "<b>Dataset:</b> <a href='https://usc.edu/iemocap/'>IEMOCAP</a>, <a href='https://zenodo.org/record/1188976'>RAVDESS</a>, <a href='https://github.com/bfelbo/DeepMoji/tree/master/data/PsychExp'>PsychExp</a><br/>\n",
        "<b>Predicting emotions:</b><br/>\n",
        "- IEMOCAP: [happy + excited, sad, angry, neutral]<br/>\n",
        "- RAVDESS: [neutral, calm, happy, sad, angry, fearful, disgust, surprised]<br/>\n",
        "- PsychExp: [joy, fear, anger, sadness, disgust, shame, guilt]<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHBgi40Tycgh"
      },
      "source": [
        "\n",
        "###Resources \n",
        "- https://ricardodeazambuja.com/deep_learning/2019/03/09/audio_and_video_google_colab/\n",
        "- https://stackoverflow.com/questions/9031783/hide-all-warnings-in-ipython\n",
        "- https://getemoji.com/\n",
        "- https://realpython.com/python-speech-recognition/\n",
        "- https://github.com/Uberi/speech_recognition#readme\n",
        "- https://www.howtogeek.com/423214/how-to-use-the-rename-command-on-linux/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj-Bbk2DyK1-"
      },
      "source": [
        "## Code preparation\n",
        "### Please run this code cell (click inside the cell and press `Ctrl + Enter`, or click on the `run icon` in the top left corner of the cell)\n",
        "\n",
        "What this does:\n",
        "\n",
        "1.   Installs the required Python packages\n",
        "2.   Clones a Github repository containing classes for Emotion Recognition models\n",
        "3.   Downloads the trained saved models\n",
        "4.   Moves the downloaded trained saved models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktVYqO43jrPs"
      },
      "source": [
        "# Step 1) Installation\n",
        "!pip install -q ffmpeg-python SpeechRecognition gTTS pydub librosa tensorflow-text\n",
        "\n",
        "# Step 2) Connect to GitHub\n",
        "!git clone https://github.com/HonzaCuhel/mer-thesis-app\n",
        "\n",
        "# Step 3) Download the models\n",
        "!wget 'https://alquist2021-data.s3.amazonaws.com/public/mer_electra_yamnet_iemocap_model+(1).h5'\n",
        "!wget 'https://alquist2021-data.s3.amazonaws.com/public/mer_trill_electra_small_model.h5'\n",
        "!wget 'https://alquist2021-data.s3.amazonaws.com/public/ter_electra_iemocap_model+(1).h5'\n",
        "!wget 'https://alquist2021-data.s3.amazonaws.com/public/ter_electra_model_psychexp.h5'\n",
        "\n",
        "# Step 4) Move the models\n",
        "!mv '/content/mer_electra_yamnet_iemocap_model+(1).h5' /content/mer-thesis-app/result_models/mer_electra_yamnet_iemocap_model.h5\n",
        "!mv '/content/mer_trill_electra_small_model.h5' /content/mer-thesis-app/result_models/mer_trill_electra_small_model.h5\n",
        "!mv '/content/ter_electra_iemocap_model+(1).h5' /content/mer-thesis-app/result_models/ter_electra_iemocap_model.h5\n",
        "!mv '/content/ter_electra_model_psychexp.h5' /content/mer-thesis-app/result_models/ter_electra_model_psychexp.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A17BHwScB7DF"
      },
      "source": [
        "## Code definition\n",
        "### Please run this code cell (click inside the cell and press `Ctrl + Enter`, or click on the `run icon` in the top left corner of the cell) as well\n",
        "\n",
        "What this does:\n",
        "\n",
        "1.   Imports the required packages\n",
        "2.   Defines some constants\n",
        "3.   Load the trained models\n",
        "4.   Defines functions for Emotion Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n00MeETqutfK"
      },
      "source": [
        "# Step 1) Imports\n",
        "import sys\n",
        "import gtts\n",
        "import os\n",
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import speech_recognition as sr\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sys.path.append('/content/mer-thesis-app/')\n",
        "from record_audio import get_audio\n",
        "from predict_emotion_tf import *\n",
        "\n",
        "\n",
        "# Step 2) Defining constants\n",
        "lang = 'en'\n",
        "dur = 11\n",
        "emoji_dict = {\"happy\":\"üòä\", \"fear\":\"üò±\", \"angry\":\"üò°\", \"sad\":\"üò¢\", \"disgust\":\"ü§Æ\", \"shame\":\"üò≥\", \"guilt\":\"üòì\", \"neutral\": \"üòê\"}\n",
        "NO = 'no'\n",
        "DEFAULT_SAMPLE_RATE = 16000\n",
        "output_file = 'output_emotion.mp3'\n",
        "\n",
        "# Step 3) Model loading\n",
        "print('Models are being loaded, it will take some time...')\n",
        "\n",
        "ser_trill_model_iemocap = TRILLSERModel(SER_TRILL_MODEL_IEMOCAP, TRILL_URL, emotions_iemocap, input_length_iemocap, SAMPLING_RATE)\n",
        "ser_trill_model_ravdess = TRILLSERModel(SER_TRILL_MODEL_RAVDESS, TRILL_URL, emotions_ravdess, input_length_ravdess, SAMPLING_RATE)\n",
        "mer_electra_trill_model_iemocap = ElectraTRILLMERModel(MER_ELECTRA_TRILL, TRILL_URL, emotions_iemocap, input_length_iemocap, SAMPLING_RATE)\n",
        "\n",
        "ser_yamnet_model_iemocap = YAMNetSERModel(SER_YAMNET_MODEL_IEMOCAP, YAMNET_URL, emotions_iemocap, input_length_iemocap, SAMPLING_RATE)\n",
        "ser_yamnet_model_ravdess = YAMNetSERModel(SER_YAMNET_MODEL_RAVDESS, YAMNET_URL, emotions_ravdess, input_length_ravdess, SAMPLING_RATE)\n",
        "mer_electra_yamnet_model_iemocap = ElectraYAMNetMERModel(MER_ELECTRA_YAMNET, YAMNET_URL, emotions_iemocap, input_length_iemocap, SAMPLING_RATE)\n",
        "\n",
        "ter_electra_model_iemocap = TERModel(TER_ELECTRA_IEMOCAP, emotions_iemocap)\n",
        "ter_electra_model_psychexp = TERModel(TER_ELECTRA_PSYCHEXP, emotion_psychexp)\n",
        "\n",
        "print('Models are loaded!')\n",
        "\n",
        "# Step 4) Definition of functions\n",
        "def get_transription(audio_file):\n",
        "  # use the audio file as the audio source\n",
        "  r = sr.Recognizer()\n",
        "  with sr.AudioFile(audio_file) as source:\n",
        "      audio = r.record(source, duration=dur)  # read the entire audio file\n",
        "  \n",
        "  # Resource: https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py\n",
        "  # Recognize speech using Google Speech Recognition\n",
        "  try:\n",
        "      text = r.recognize_google(audio, language=lang)\n",
        "  except sr.UnknownValueError:\n",
        "      print(\"Google Speech Recognition could not understand audio\")\n",
        "      return \"\"\n",
        "  except sr.RequestError as e:\n",
        "      print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
        "      return \"\"\n",
        "  \n",
        "  return text\n",
        "\n",
        "def predict_emotion(audio_file, print_intro=True):\n",
        "  if print_intro:\n",
        "    print('Welcome to the Multimodal Speech Emotion Recognizer app from audio and text!')\n",
        "    print('-'*80)\n",
        "    print('Help:')\n",
        "    print(' - record a speech and the program will recognize your emotion')\n",
        "\n",
        "  print('Recognizing emotion...')\n",
        "  # Recognize the emotion\n",
        "  text = get_transription(audio_file)\n",
        "\n",
        "  # TRILL predictions\n",
        "  ser_trill_iemocap = ser_trill_model_iemocap.predict_emotion(audio_file)\n",
        "  ser_trill_ravdess = ser_trill_model_ravdess.predict_emotion(audio_file)\n",
        "  mer_trill_electra = mer_electra_trill_model_iemocap.predict_emotion(text, audio_file)\n",
        "\n",
        "  # Yamnet predictions\n",
        "  ser_yamnet_iemocap = ser_yamnet_model_iemocap.predict_emotion(audio_file)\n",
        "  ser_yamnet_ravdess = ser_yamnet_model_ravdess.predict_emotion(audio_file)\n",
        "  mer_yamnet_electra = mer_electra_yamnet_model_iemocap.predict_emotion(text, audio_file)\n",
        "\n",
        "  # TER Electra predictions\n",
        "  ter_electra_iemocap = ter_electra_model_iemocap.predict_emotion(text)\n",
        "  ter_electra_psychexp = ter_electra_model_psychexp.predict_emotion(text)\n",
        "\n",
        "  print('\\n' + '='*60)\n",
        "  print(f'\\nYou\\'ve said: {text}.\\n')\n",
        "  print(\"Audio's waveform:\")\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.plot(librosa.load(audio_file)[0])\n",
        "  plt.title(f'Audio\\'s waveform (sample rate {round(DEFAULT_SAMPLE_RATE/1000)}kHz)')\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Amplitude')\n",
        "  plt.show()\n",
        "  print('='*60)\n",
        "  print(\"Predictions:\")\n",
        "  print('-'*40)\n",
        "  print('TRILL models:')\n",
        "  print(f'MER Electra TRILL (IEMOCAP): {mer_trill_electra}')\n",
        "  print(f'SER TRILL (IEMOCAP): {ser_trill_iemocap}')\n",
        "  print(f'SER TRILL (RAVDESS): {ser_trill_ravdess}')\n",
        "  print('-'*40)\n",
        "  print('YAMNet models:')\n",
        "  print(f'MER Electra YAMNet (IEMOCAP): {mer_yamnet_electra}')\n",
        "  print(f'SER YAMNet (IEMOCAP): {ser_yamnet_iemocap}')\n",
        "  print(f'SER YAMNet (RAVDESS): {ser_yamnet_ravdess}')\n",
        "  print('-'*40)\n",
        "  print('Only text - Electra small')\n",
        "  print(f'TER Electra small (IEMOCAP): {ter_electra_iemocap}')\n",
        "  print(f'TER Electra small (PsychExp): {ter_electra_psychexp}')\n",
        "  print('='*60)\n",
        "\n",
        "  return mer_trill_electra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETpnzv33QOzP"
      },
      "source": [
        "### Record a speech\n",
        "\n",
        "Here you can record a sample of your speech. To record just execute the next cell by either hitting the `run icon` or by clicking inside of the cell and then press `Ctrl + Enter`. After you've sad something click on the button `Stop recording` to stop recording. \n",
        "\n",
        "<b>WARNING: ONLY 11 SECONDS OF YOUR SPEECH WILL BE USED, SO IF YOU WILL SPEAK LONGER THE AUDIO FILE WILL BE TRUNCATED, IF YOU WILL SPEAK LESS, IT IS FINE, THE AUDIO RECORDING WILL BE PADDED.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPTSXUW9u3Ak"
      },
      "source": [
        "audio, sample_rate, audio_file = get_audio()\n",
        "\n",
        "print('Speech recorded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhbvAYwJP2Ym"
      },
      "source": [
        "#### Emotion recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ7Fcv6WJx2A"
      },
      "source": [
        "pred_emotion = predict_emotion(audio_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEacJF59wbM4"
      },
      "source": [
        "### Audio uploading\n",
        "\n",
        "You can test out the models by uploading `.wav` audio files and the models will try to predict emotions from them. Try it! \n",
        "\n",
        "<b>WARNING: ONLY 11 SECONDS OF THE AUDIO FILES WILL BE USED, SO IF YOU WILL SPEAK LONGER THE AUDIO FILE WILL BE TRUNCATED, IF YOU WILL SPEAK LESS, IT IS FINE, THE AUDIO RECORDING WILL BE PADDED.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btlhd4b5wmjp"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "# Wav files counter \n",
        "i_num = 1\n",
        "\n",
        "for uf in uploaded.keys():\n",
        "  if '.wav' in uf:\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=uf, length=len(uploaded[uf])))\n",
        "    \n",
        "    print('*'*80)\n",
        "    print(f'{i_num}) RESULTS FOR {uf}:')\n",
        "    # Predict the emotion\n",
        "    pred_emotion = predict_emotion(uf, print_intro=False)\n",
        "    print('*'*80)\n",
        "\n",
        "    # Actualize counter\n",
        "    i_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66McRKEM_N5i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}